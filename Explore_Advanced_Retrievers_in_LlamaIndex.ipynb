{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeyDs2n1bc/+ZE7W1nDRYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAbdallah21/Advanced-Retrievers-in-LlamaIndex/blob/main/Explore_Advanced_Retrievers_in_LlamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WtlKzpVOqI2x"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index-core \\\n",
        "    llama-index-llms-openai \\\n",
        "    llama-index-embeddings-openai \\\n",
        "    llama-index-retrievers-bm25 \\\n",
        "    sentence-transformers \\\n",
        "    rank-bm25 \\\n",
        "    pystemmer \\\n",
        "    llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title imports\n",
        "import os\n",
        "import json\n",
        "import openai\n",
        "from typing import List, Dict\n",
        "import asyncio\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "#LLamaIndex imports\n",
        "\n",
        "from llama_index.core import(\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    Document,\n",
        "    Settings,\n",
        "    DocumentSummaryIndex,\n",
        "    SummaryIndex,\n",
        "    KeywordTableIndex\n",
        ")\n",
        "\n",
        "\n",
        "from llama_index.core.retrievers import (\n",
        "    BaseRetriever,\n",
        "    VectorIndexRetriever,\n",
        "    AutoMergingRetriever,\n",
        "    RecursiveRetriever,\n",
        "    QueryFusionRetriever\n",
        ")\n",
        "\n",
        "from llama_index.core.indices.document_summary import (\n",
        "    DocumentSummaryIndexLLMRetriever,\n",
        "    DocumentSummaryIndexEmbeddingRetriever,\n",
        ")\n",
        "\n",
        "from llama_index.core.node_parser import SentenceSplitter, HierarchicalNodeParser\n",
        "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "from llama_index.core.embeddings import BaseEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openai import OpenAI # Import OpenAI class\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.core.vector_stores import SimpleVectorStore\n",
        "\n",
        "\n",
        "\n",
        "# Advanced retriever imports\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Statistical libraries for fusion techniques\n",
        "try:\n",
        "    from scipy import stats\n",
        "    SCIPY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SCIPY_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è scipy not available - some advanced fusion features will be limited\")\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjHDhnXEFS9p",
        "outputId": "a6990cbc-138e-4d98-a0c7-c2facfed0f98"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All imports successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title OpenAi() key setup\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "8BXqSRVHK07Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize Embedding Model\n",
        "print(\"üîß Initializing HuggingFace embeddings...\")\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "print(\"‚úÖ HuggingFace embeddings initialized!\")\n",
        "\n",
        "# Setup with watsonx.ai\n",
        "print(\"üîß Initializing openai LLM...\")\n",
        "\n",
        "llm = OpenAI()  # Instantiate the OpenAI class\n",
        "\n",
        "# Configure global settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "print(\"‚úÖ OpenAI LLM and embeddings configured!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z83eQFc1Jw_y",
        "outputId": "60988fd1-bec9-467c-f65f-4718c949e717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initializing HuggingFace embeddings...\n",
            "‚úÖ HuggingFace embeddings initialized!\n",
            "üîß Initializing openai LLM...\n",
            "‚úÖ OpenAI LLM and embeddings configured!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sample data\n",
        "# Sample data for the lab - AI/ML focused documents\n",
        "SAMPLE_DOCUMENTS = [\n",
        "    \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
        "    \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
        "    \"Natural language processing enables computers to understand, interpret, and generate human language.\",\n",
        "    \"Computer vision allows machines to interpret and understand visual information from the world.\",\n",
        "    \"Reinforcement learning is a type of machine learning where agents learn to make decisions through rewards and penalties.\",\n",
        "    \"Supervised learning uses labeled training data to learn a mapping from inputs to outputs.\",\n",
        "    \"Unsupervised learning finds hidden patterns in data without labeled examples.\",\n",
        "    \"Transfer learning leverages knowledge from pre-trained models to improve performance on new tasks.\",\n",
        "    \"Generative AI can create new content including text, images, code, and more.\",\n",
        "    \"Large language models are trained on vast amounts of text data to understand and generate human-like text.\"\n",
        "]\n",
        "\n",
        "# Consistent query examples used throughout the lab\n",
        "DEMO_QUERIES = {\n",
        "    \"basic\": \"What is machine learning?\",\n",
        "    \"technical\": \"neural networks deep learning\",\n",
        "    \"learning_types\": \"different types of learning\",\n",
        "    \"advanced\": \"How do neural networks work in deep learning?\",\n",
        "    \"applications\": \"What are the applications of AI?\",\n",
        "    \"comprehensive\": \"What are the main approaches to machine learning?\",\n",
        "    \"specific\": \"supervised learning techniques\"\n",
        "}\n",
        "\n",
        "print(f\"üìÑ Loaded {len(SAMPLE_DOCUMENTS)} sample documents\")\n",
        "print(f\"üîç Prepared {len(DEMO_QUERIES)} consistent demo queries\")\n",
        "for i, doc in enumerate(SAMPLE_DOCUMENTS[:3], 1):\n",
        "    print(f\"{i}. {doc}\")\n",
        "print(\"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwKsfLrXLjCa",
        "outputId": "d388e713-a16a-41a4-a337-a979440a6458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Loaded 10 sample documents\n",
            "üîç Prepared 7 consistent demo queries\n",
            "1. Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\n",
            "2. Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\n",
            "3. Natural language processing enables computers to understand, interpret, and generate human language.\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initiallizing Lab Class\n",
        "class AdvancedRetrievalsLab:\n",
        "    def __init__(self):\n",
        "      print(\"üîß Initializing Advanced Retrievals Lab...\")\n",
        "      self.documents = [Document(text=text) for text in SAMPLE_DOCUMENTS]\n",
        "      self.nodes = SentenceSplitter().get_nodes_from_documents(self.documents)\n",
        "\n",
        "\n",
        "      print(\"üìä Creating indexes...\")\n",
        "      self.vector_index = VectorStoreIndex(self.nodes)\n",
        "      self.document_summary_index = DocumentSummaryIndex.from_documents(self.documents)\n",
        "      self.keyword_table_index = KeywordTableIndex.from_documents(self.documents)\n",
        "      print(\"‚úÖ Advanced Retrievers Lab Initialized!\")\n",
        "      print(f\"üìÑ Loaded {len(self.documents)} documents\")\n",
        "      print(f\"üî¢ Created {len(self.nodes)} nodes\")\n",
        "\n",
        "\n",
        "\n",
        "lab = AdvancedRetrievalsLab()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgIlTRACNXpO",
        "outputId": "d18563e6-4a7a-43fc-a23a-dee5707c09f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initializing Advanced Retrievals Lab...\n",
            "üìä Creating indexes...\n",
            "current doc id: 2464019a-ee5c-4609-b2d3-303d0347c461\n",
            "current doc id: 6b46271b-8ce9-4484-af3f-4cb52d6343e7\n",
            "current doc id: 866aa372-7918-43e2-bd14-2100b2182900\n",
            "current doc id: a5787c28-8f50-4e6a-92b2-4bf574253c41\n",
            "current doc id: 88646138-4dbc-4377-adb9-dd7bb2766f88\n",
            "current doc id: 093d3dfc-2925-4fb3-8c86-687bd5c7d700\n",
            "current doc id: ca367a00-f82e-4a24-bf35-d73325b51daa\n",
            "current doc id: ed72e3ff-3527-4c16-89e4-daf6bea9f893\n",
            "current doc id: 57824d2f-b511-46b1-9667-a3df6d9a4489\n",
            "current doc id: d0bf3f6a-472a-4e14-a3d8-8a5880c39140\n",
            "‚úÖ Advanced Retrievers Lab Initialized!\n",
            "üìÑ Loaded 10 documents\n",
            "üî¢ Created 10 nodes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Vector Index Retriever\n",
        "print (\"=\" * 60)\n",
        "print (\"üîß Vector Index Retriever\")\n",
        "print (\"=\" * 60)\n",
        "\n",
        "vector_retriever = VectorIndexRetriever(\n",
        "    index=lab.vector_index,\n",
        "    similarity_top_k=3\n",
        ")\n",
        "\n",
        "# Another way of retrieval\n",
        "# alt_retriever = lab.vector_index.as_retriever(similarity_top_k = 3)\n",
        "\n",
        "query = DEMO_QUERIES[\"basic\"]\n",
        "nodes = vector_retriever.retrieve(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Retrieved {len(nodes)} nodes:\")\n",
        "for i, node in enumerate(nodes,1):\n",
        "  print(f\"{i}. Score: {node.score:.4f}\")\n",
        "  print(f\"    Text: {node.text[:100]}...\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BRB4f-ISd7t",
        "outputId": "53faab68-2c48-4477-b915-803a0a02646d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üîß Vector Index Retriever\n",
            "============================================================\n",
            "Query: What is machine learning?\n",
            "Retrieved 3 nodes:\n",
            "1. Score: 0.8700\n",
            "    Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
            "\n",
            "2. Score: 0.7644\n",
            "    Text: Reinforcement learning is a type of machine learning where agents learn to make decisions through re...\n",
            "\n",
            "3. Score: 0.6979\n",
            "    Text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs....\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BM25 Retriever - Advanced Keyword-Based Search\n",
        "print (\"=\" * 60)\n",
        "print (\"üîß BM25 Retriever\")\n",
        "print (\"=\" * 60)\n",
        "\n",
        "try:\n",
        "  import Stemmer\n",
        "\n",
        "  bm_25_retriever = BM25Retriever.from_defaults(\n",
        "      nodes = lab.nodes,\n",
        "      similarity_top_k = 3,\n",
        "      stemmer = Stemmer.Stemmer('english'),\n",
        "      language = \"english\"\n",
        "  )\n",
        "\n",
        "  query = DEMO_QUERIES[\"technical\"]\n",
        "  nodes = bm_25_retriever.retrieve(query)\n",
        "\n",
        "  print (f\"Query: {query}\")\n",
        "  print(\"BM25 analyzes exact keyword matches with sophisticated scoring\")\n",
        "  print (f\"Retrieved {len(nodes)} nodes:\")\n",
        "\n",
        "  for i, node in enumerate(nodes , 1):\n",
        "    score = node.score if hasattr(node, 'score') and node.score else 0\n",
        "    print(f\"{i}. BM25 Score: {score:.4f}\")\n",
        "    print(f\"   Text: {node.text[:100]}...\")\n",
        "\n",
        "\n",
        "    text_lower = node.text.lower()\n",
        "    query_terms = query.lower().split()\n",
        "    found_terms = [term for term in query_terms if term in text_lower]\n",
        "    if found_terms:\n",
        "      print(f\"   Matched terms: {found_terms}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "  print(\"BM25 vs TF-IDF Comparison:\")\n",
        "  print(\"TF-IDF Problem: Linear term frequency scaling\")\n",
        "  print(\"  Example: 10 occurrences ‚Üí score of 10, 100 occurrences ‚Üí score of 100\")\n",
        "  print(\"BM25 Solution: Saturation function\")\n",
        "  print(\"  Example: 10 occurrences ‚Üí high score, 100 occurrences ‚Üí slightly higher score\")\n",
        "  print()\n",
        "  print(\"TF-IDF Problem: No document length consideration\")\n",
        "  print(\"  Example: Long documents dominate results\")\n",
        "  print(\"BM25 Solution: Length normalization (b parameter)\")\n",
        "  print(\"  Example: Scores adjusted based on document length vs. average\")\n",
        "  print()\n",
        "  print(\"Key BM25 Parameters:\")\n",
        "  print(\"- k1 ‚âà 1.2: Term frequency saturation (how quickly scores plateau)\")\n",
        "  print(\"- b ‚âà 0.75: Document length normalization (0=none, 1=full)\")\n",
        "  print(\"- IDF weighting: Rare terms get higher scores\")\n",
        "\n",
        "\n",
        "except ImportError:\n",
        "\n",
        "  print(\"‚ö†Ô∏è BM25Retriever requires 'pip install PyStemmer'\")\n",
        "  print(\"Demonstrating BM25 concepts with fallback vector search...\")\n",
        "  fallback_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
        "  query = DEMO_QUERIES[\"technical\"]\n",
        "  nodes = fallback_retriever.retrieve(query)\n",
        "\n",
        "  print(f\"Query: {query}\")\n",
        "  print(\"(Using vector fallback to demonstrate BM25 concepts)\")\n",
        "\n",
        "  for i, node in enumerate(nodes, 1):\n",
        "      print(f\"{i}. Vector Score: {node.score:.4f}\")\n",
        "      print(f\"   Text: {node.text[:100]}...\")\n",
        "\n",
        "      # Demonstrate TF-IDF concept manually\n",
        "      text_lower = node.text.lower()\n",
        "      query_terms = query.lower().split()\n",
        "      found_terms = [term for term in query_terms if term in text_lower]\n",
        "\n",
        "      if found_terms:\n",
        "          print(f\"   ‚Üí BM25 would boost this result for terms: {found_terms}\")\n",
        "      print()\n",
        "\n",
        "  print(\"BM25 Concept Demonstration:\")\n",
        "  print(\"1. TF-IDF Foundation:\")\n",
        "  print(\"   - Term Frequency: How often words appear in document\")\n",
        "  print(\"   - Inverse Document Frequency: How rare words are across collection\")\n",
        "  print(\"   - TF-IDF = TF √ó IDF (balances frequency vs rarity)\")\n",
        "  print()\n",
        "  print(\"2. BM25 Improvements:\")\n",
        "  print(\"   - Saturation: Prevents over-scoring repeated terms\")\n",
        "  print(\"   - Length normalization: Prevents long document bias\")\n",
        "  print(\"   - Tunable parameters: k1 (saturation) and b (length adjustment)\")\n",
        "  print()\n",
        "  print(\"3. Real-world Usage:\")\n",
        "  print(\"   - Elasticsearch default scoring function\")\n",
        "  print(\"   - Apache Lucene/Solr standard\")\n",
        "  print(\"   - Used in 83% of text-based recommender systems\")\n",
        "  print(\"   - Developed by Robertson & Sp√§rck Jones at City University London\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtwbZboKUtf2",
        "outputId": "8e8322c1-0cef-4ae9-e6d2-e7d94475887f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:bm25s:Building index from IDs objects\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üîß BM25 Retriever\n",
            "============================================================\n",
            "Query: neural networks deep learning\n",
            "BM25 analyzes exact keyword matches with sophisticated scoring\n",
            "Retrieved 3 nodes:\n",
            "1. BM25 Score: 2.5203\n",
            "   Text: Deep learning uses neural networks with multiple layers to model and understand complex patterns in ...\n",
            "   Matched terms: ['neural', 'networks', 'deep', 'learning']\n",
            "\n",
            "2. BM25 Score: 0.3372\n",
            "   Text: Reinforcement learning is a type of machine learning where agents learn to make decisions through re...\n",
            "   Matched terms: ['learning']\n",
            "\n",
            "3. BM25 Score: 0.3024\n",
            "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
            "   Matched terms: ['learning']\n",
            "\n",
            "BM25 vs TF-IDF Comparison:\n",
            "TF-IDF Problem: Linear term frequency scaling\n",
            "  Example: 10 occurrences ‚Üí score of 10, 100 occurrences ‚Üí score of 100\n",
            "BM25 Solution: Saturation function\n",
            "  Example: 10 occurrences ‚Üí high score, 100 occurrences ‚Üí slightly higher score\n",
            "\n",
            "TF-IDF Problem: No document length consideration\n",
            "  Example: Long documents dominate results\n",
            "BM25 Solution: Length normalization (b parameter)\n",
            "  Example: Scores adjusted based on document length vs. average\n",
            "\n",
            "Key BM25 Parameters:\n",
            "- k1 ‚âà 1.2: Term frequency saturation (how quickly scores plateau)\n",
            "- b ‚âà 0.75: Document length normalization (0=none, 1=full)\n",
            "- IDF weighting: Rare terms get higher scores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Document Summary Index Retrievers\n",
        "print (\"=\" * 60)\n",
        "print (\"üîß Document Summary Index Retrievers\")\n",
        "print (\"=\" * 60)\n",
        "\n",
        "document_summary_retriever_llm = DocumentSummaryIndexLLMRetriever(\n",
        "    lab.document_summary_index,\n",
        "    choice_top_k = 3\n",
        ")\n",
        "\n",
        "document_summary_retriever_embedding = DocumentSummaryIndexEmbeddingRetriever(\n",
        "    lab.document_summary_index,\n",
        "    similarity_top_k = 3\n",
        ")\n",
        "\n",
        "query = DEMO_QUERIES[\"learning_types\"]\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "print(\"\\nA) LLM-based Document Summary Retriever:\")\n",
        "print(\"Uses LLM to select relevant documents based on summaries\")\n",
        "\n",
        "try:\n",
        "  nodes_llm = document_summary_retriever_llm.retrieve(query)\n",
        "  print(f\"Retrieved {len(nodes_llm)} nodes\")\n",
        "  for i, node in enumerate(nodes_llm[:2],1):\n",
        "    print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Document summary)\")\n",
        "    print(f\"   Text: {node.text[:80]}...\")\n",
        "    print()\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"LLM-based retrieval demo: {str(e)[:100]}...\")\n",
        "\n",
        "print(\"B) Embedding-based Document Summary Retriever:\")\n",
        "print(\"Uses vector similarity between query and document summaries\")\n",
        "try:\n",
        "    nodes_emb = document_summary_retriever_embedding.retrieve(query)\n",
        "    print(f\"Retrieved {len(nodes_emb)} nodes\")\n",
        "    for i, node in enumerate(nodes_emb[:2], 1):\n",
        "        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Document summary)\")\n",
        "        print(f\"   Text: {node.text[:80]}...\")\n",
        "        print()\n",
        "except Exception as e:\n",
        "    print(f\"Embedding-based retrieval demo: {str(e)[:100]}...\")\n",
        "\n",
        "print(\"Document Summary Index workflow:\")\n",
        "print(\"1. Generates summaries for each document using LLM\")\n",
        "print(\"2. Uses summaries to select relevant documents\")\n",
        "print(\"3. Returns full content from selected documents\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yqrTyy5dGY0",
        "outputId": "a6042314-355a-4e13-c028-6bdfd22f4cd0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üîß Document Summary Index Retrievers\n",
            "============================================================\n",
            "Query: different types of learning\n",
            "\n",
            "A) LLM-based Document Summary Retriever:\n",
            "Uses LLM to select relevant documents based on summaries\n",
            "Retrieved 3 nodes\n",
            "1. Score: 9.0000\n",
            "   Text: Machine learning is a subset of artificial intelligence that focuses on algorith...\n",
            "\n",
            "2. Score: 8.0000\n",
            "   Text: Deep learning uses neural networks with multiple layers to model and understand ...\n",
            "\n",
            "B) Embedding-based Document Summary Retriever:\n",
            "Uses vector similarity between query and document summaries\n",
            "Retrieved 3 nodes\n",
            "1. (Document summary)\n",
            "   Text: Unsupervised learning finds hidden patterns in data without labeled examples....\n",
            "\n",
            "2. (Document summary)\n",
            "   Text: Supervised learning uses labeled training data to learn a mapping from inputs to...\n",
            "\n",
            "Document Summary Index workflow:\n",
            "1. Generates summaries for each document using LLM\n",
            "2. Uses summaries to select relevant documents\n",
            "3. Returns full content from selected documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Auto Merging Retriever - Hierarchical Context Preservation\n",
        "print (\"=\" * 60)\n",
        "print (\"üîß Auto Merging Retriever\")\n",
        "print (\"=\" * 60)\n",
        "\n",
        "node_parser = HierarchicalNodeParser.from_defaults(\n",
        "    chunk_sizes = [512, 256 , 128]\n",
        ")\n",
        "\n",
        "hier_nodes = node_parser.get_nodes_from_documents(lab.documents)\n",
        "\n",
        "docstore = SimpleDocumentStore()\n",
        "docstore.add_documents(hier_nodes)\n",
        "\n",
        "storage_context = StorageContext.from_defaults(docstore = docstore)\n",
        "\n",
        "base_index = VectorStoreIndex(hier_nodes, storage_context = storage_context)\n",
        "base_retriever = base_index.as_retriever(similarity_top_k = 6)\n",
        "\n",
        "auto_merging_retriever = AutoMergingRetriever(\n",
        "    base_retriever,\n",
        "    node_parser,\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "\n",
        "query = DEMO_QUERIES[\"advanced\"]  # \"How do neural networks work in deep learning?\"\n",
        "nodes = auto_merging_retriever.retrieve(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Auto-merged to {len(nodes)} nodes\")\n",
        "for i, node in enumerate(nodes[:3], 1):\n",
        "    print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Auto-merged)\")\n",
        "    print(f\"   Text: {node.text[:120]}...\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "6bGbaYrthmIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf288c5-da68-4bb1-e4b2-bf054bcf7235"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üîß Auto Merging Retriever\n",
            "============================================================\n",
            "Query: How do neural networks work in deep learning?\n",
            "Auto-merged to 2 nodes\n",
            "1. Score: 0.8570\n",
            "   Text: Deep learning uses neural networks with multiple layers to model and understand complex patterns in data....\n",
            "\n",
            "2. Score: 0.6956\n",
            "   Text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs....\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Recursive Retriever\n",
        "print (\"=\" * 60)\n",
        "print (\"üîß Recursive Retriever\")\n",
        "print (\"=\" * 60)\n",
        "\n",
        "\n",
        "docs_with_refs = []\n",
        "for i, doc in enumerate(lab.documents):\n",
        "    ref_doc = Document(\n",
        "        text = doc.text,\n",
        "        metadata = {\n",
        "            \"doc_id\" : f\"doc_{i}\",\n",
        "            \"references\" : [f\"doc_{j}\" for j in range(len(lab.documents)) if j != i][:2]\n",
        "        }\n",
        "    )\n",
        "    docs_with_refs.append(ref_doc)\n",
        "\n",
        "\n",
        "ref_index = VectorStoreIndex.from_documents(docs_with_refs)\n",
        "\n",
        "retriever_dict = {\n",
        "    f\"doc_{i}\": ref_index.as_retriever(similarity_top_k=1)\n",
        "    for i in range(len(docs_with_refs))\n",
        "}\n",
        "\n",
        "base_retriever = ref_index.as_retriever(similarity_top_k=2)\n",
        "\n",
        "retriever_dict[\"vector\"] = base_retriever\n",
        "\n",
        "recursive_retriever = RecursiveRetriever(\n",
        "    \"vector\",\n",
        "    retriever_dict=retriever_dict,\n",
        "    query_engine_dict={},\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "query = DEMO_QUERIES[\"applications\"]  # \"What are the applications of AI?\"\n",
        "\n",
        "\n",
        "nodes = recursive_retriever.retrieve(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Recursively retrieved {len(nodes)} nodes\")\n",
        "\n",
        "for i, node in enumerate(nodes[:3], 1):\n",
        "  print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Recursive)\")\n",
        "  print(f\"   Text: {node.text[:100]}...\")\n",
        "  print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhIwe8RayzIX",
        "outputId": "8059961d-2da7-48aa-b0ba-78cb33438a78"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üîß Recursive Retriever\n",
            "============================================================\n",
            "\u001b[1;3;34mRetrieving with query id None: What are the applications of AI?\n",
            "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\n",
            "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Natural language processing enables computers to understand, interpret, and generate human language.\n",
            "\u001b[0mQuery: What are the applications of AI?\n",
            "Recursively retrieved 2 nodes\n",
            "1. Score: 0.6907\n",
            "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
            "\n",
            "2. Score: 0.6500\n",
            "   Text: Natural language processing enables computers to understand, interpret, and generate human language....\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query Fusion Retriever\n",
        "print(\"=\" * 60)\n",
        "print(\"6. QUERY FUSION RETRIEVER - OVERVIEW\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create base retriever\n",
        "base_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
        "\n",
        "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
        "print(f\"Query: {query}\")\n",
        "print(\"QueryFusionRetriever generates multiple query variations and fuses results\")\n",
        "print(\"using one of three sophisticated fusion modes.\")\n",
        "\n",
        "print(\"\\nOverview of Fusion Modes:\")\n",
        "print(\"1. RECIPROCAL_RERANK: Uses reciprocal rank fusion (most robust)\")\n",
        "print(\"2. RELATIVE_SCORE: Preserves score magnitudes (most interpretable)\")\n",
        "print(\"3. DIST_BASED_SCORE: Statistical normalization (most sophisticated)\")\n",
        "\n",
        "print(\"\\nDemonstration workflow:\")\n",
        "print(\"Each subsection below explores one fusion mode in detail with:\")\n",
        "print(\"- Theoretical explanation of the fusion method\")\n",
        "print(\"- Live demonstration using QueryFusionRetriever\")\n",
        "print(\"- Manual implementation showing the underlying mathematics\")\n",
        "print(\"- Use case recommendations and trade-offs\")\n",
        "\n",
        "print(f\"\\nUsing consistent test query throughout: '{query}'\")\n",
        "print(\"This allows direct comparison of how each fusion mode handles the same input.\")\n",
        "\n",
        "print(\"\\nProceed to subsections 6.1, 6.2, and 6.3 for detailed demonstrations...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvwssZIJ1yQP",
        "outputId": "0f1cb6ec-e4d7-4c47-fec7-c75c5ee30aa4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "6. QUERY FUSION RETRIEVER - OVERVIEW\n",
            "============================================================\n",
            "Query: What are the main approaches to machine learning?\n",
            "QueryFusionRetriever generates multiple query variations and fuses results\n",
            "using one of three sophisticated fusion modes.\n",
            "\n",
            "Overview of Fusion Modes:\n",
            "1. RECIPROCAL_RERANK: Uses reciprocal rank fusion (most robust)\n",
            "2. RELATIVE_SCORE: Preserves score magnitudes (most interpretable)\n",
            "3. DIST_BASED_SCORE: Statistical normalization (most sophisticated)\n",
            "\n",
            "Demonstration workflow:\n",
            "Each subsection below explores one fusion mode in detail with:\n",
            "- Theoretical explanation of the fusion method\n",
            "- Live demonstration using QueryFusionRetriever\n",
            "- Manual implementation showing the underlying mathematics\n",
            "- Use case recommendations and trade-offs\n",
            "\n",
            "Using consistent test query throughout: 'What are the main approaches to machine learning?'\n",
            "This allows direct comparison of how each fusion mode handles the same input.\n",
            "\n",
            "Proceed to subsections 6.1, 6.2, and 6.3 for detailed demonstrations...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RECIPROCAL RANK FUSION\n",
        "print(\"=\" * 60)\n",
        "print(\"6.1 RECIPROCAL RANK FUSION MODE DEMONSTRATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "base_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
        "print(\"Testing QueryFusionRetriever with reciprocal_rerank mode:\")\n",
        "print(\"This demonstrates how RRF works within the query fusion framework\")\n",
        "\n",
        "\n",
        "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
        "\n",
        "rrf_query_fusion = QueryFusionRetriever(\n",
        "        [base_retriever],\n",
        "        similarity_top_k=3,\n",
        "        num_queries=3,\n",
        "        mode=\"reciprocal_rerank\",\n",
        "        use_async=False,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(\"QueryFusionRetriever will:\")\n",
        "print(\"1. Generate query variations using LLM\")\n",
        "print(\"2. Retrieve results for each variation\")\n",
        "print(\"3. Apply Reciprocal Rank Fusion\")\n",
        "\n",
        "nodes = rrf_query_fusion.retrieve(query)\n",
        "\n",
        "print(f\"\\nRRF Query Fusion Results:\")\n",
        "for i, node in enumerate(nodes, 1):\n",
        "    print(f\"{i}. Final RRF Score: {node.score:.4f}\")\n",
        "    print(f\"   Text: {node.text[:100]}...\")\n",
        "    print()\n",
        "\n",
        "print(\"RRF Benefits in Query Fusion Context:\")\n",
        "print(\"- Automatically handles query variations of different quality\")\n",
        "print(\"- No bias toward queries that return higher raw scores\")\n",
        "print(\"- Stable performance across diverse query formulations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w37VYqe4x38",
        "outputId": "59e87ecd-739d-4d4b-ca8d-8ab230b84134"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "6.1 RECIPROCAL RANK FUSION MODE DEMONSTRATION\n",
            "============================================================\n",
            "Testing QueryFusionRetriever with reciprocal_rerank mode:\n",
            "This demonstrates how RRF works within the query fusion framework\n",
            "\n",
            "Query: What are the main approaches to machine learning?\n",
            "QueryFusionRetriever will:\n",
            "1. Generate query variations using LLM\n",
            "2. Retrieve results for each variation\n",
            "3. Apply Reciprocal Rank Fusion\n",
            "Generated queries:\n",
            "1. What are the differences between supervised and unsupervised machine learning?\n",
            "2. How do reinforcement learning algorithms work in the context of machine learning?\n",
            "\n",
            "RRF Query Fusion Results:\n",
            "1. Final RRF Score: 0.0492\n",
            "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
            "\n",
            "2. Final RRF Score: 0.0492\n",
            "   Text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs....\n",
            "\n",
            "3. Final RRF Score: 0.0167\n",
            "   Text: Reinforcement learning is a type of machine learning where agents learn to make decisions through re...\n",
            "\n",
            "RRF Benefits in Query Fusion Context:\n",
            "- Automatically handles query variations of different quality\n",
            "- No bias toward queries that return higher raw scores\n",
            "- Stable performance across diverse query formulations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RELATIVE SCORE FUSION\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"6.2 RELATIVE SCORE FUSION MODE DEMONSTRATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "base_retriever = lab.vector_index.as_retriever(similarity_top_k=5)\n",
        "\n",
        "print(\"Testing QueryFusionRetriever with relative_score mode:\")\n",
        "print(\"This mode preserves score magnitudes while normalizing across query variations\")\n",
        "\n",
        "# Use the same query for consistency across all fusion modes\n",
        "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
        "\n",
        "try:\n",
        "    # Create query fusion retriever with relative score mode\n",
        "    rel_score_fusion = QueryFusionRetriever(\n",
        "        [base_retriever],\n",
        "        similarity_top_k=3,\n",
        "        num_queries=3,\n",
        "        mode=\"relative_score\",\n",
        "        use_async=False,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(\"QueryFusionRetriever with relative_score will:\")\n",
        "    print(\"1. Generate query variations\")\n",
        "    print(\"2. Normalize scores within each variation (score/max_score)\")\n",
        "    print(\"3. Combine normalized scores\")\n",
        "\n",
        "    nodes = rel_score_fusion.retrieve(query)\n",
        "\n",
        "    print(f\"\\nRelative Score Fusion Results:\")\n",
        "    for i, node in enumerate(nodes, 1):\n",
        "        print(f\"{i}. Combined Relative Score: {node.score:.4f}\")\n",
        "        print(f\"   Text: {node.text[:100]}...\")\n",
        "        print()\n",
        "\n",
        "    print(\"Relative Score Benefits in Query Fusion:\")\n",
        "    print(\"- Preserves confidence information from embedding model\")\n",
        "    print(\"- Ensures fair contribution from each query variation\")\n",
        "    print(\"- More interpretable than rank-only methods\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"QueryFusionRetriever error: {e}\")\n",
        "    print(\"Demonstrating Relative Score concept manually...\")\n",
        "\n",
        "    # Manual demonstration with query variations derived from the main query\n",
        "    query_variations = [\n",
        "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
        "        \"machine learning approaches and methods\",\n",
        "        \"different ML techniques and algorithms\"\n",
        "    ]\n",
        "\n",
        "    print(\"Manual Relative Score Fusion with Query Variations:\")\n",
        "    all_results = {}\n",
        "    query_max_scores = []\n",
        "\n",
        "    # Step 1: Get results and find max scores for each query\n",
        "    for i, query_var in enumerate(query_variations):\n",
        "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
        "        nodes = base_retriever.retrieve(query_var)\n",
        "        scores = [node.score or 0 for node in nodes]\n",
        "        max_score = max(scores) if scores else 1.0\n",
        "        query_max_scores.append(max_score)\n",
        "\n",
        "        print(f\"Max score for this query: {max_score:.4f}\")\n",
        "\n",
        "        # Store results with normalization info\n",
        "        for node in nodes:\n",
        "            node_id = node.node.node_id\n",
        "            original_score = node.score or 0\n",
        "            normalized_score = original_score / max_score if max_score > 0 else 0\n",
        "\n",
        "            if node_id not in all_results:\n",
        "                all_results[node_id] = {\n",
        "                    'node': node,\n",
        "                    'combined_score': 0,\n",
        "                    'contributions': []\n",
        "                }\n",
        "\n",
        "            all_results[node_id]['combined_score'] += normalized_score\n",
        "            all_results[node_id]['contributions'].append({\n",
        "                'query': i,\n",
        "                'original': original_score,\n",
        "                'normalized': normalized_score\n",
        "            })\n",
        "\n",
        "    # Step 2: Sort by combined relative score\n",
        "    sorted_results = sorted(\n",
        "        all_results.values(),\n",
        "        key=lambda x: x['combined_score'],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\nCombined Relative Score Results (top 3):\")\n",
        "    for i, result in enumerate(sorted_results[:3], 1):\n",
        "        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n",
        "        print(f\"   Score breakdown:\")\n",
        "        for contrib in result['contributions']:\n",
        "            print(f\"     Query {contrib['query']}: {contrib['original']:.3f} ‚Üí {contrib['normalized']:.3f}\")\n",
        "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
        "        print()\n",
        "\n",
        "    print(\"Relative Score Normalization Process:\")\n",
        "    print(\"1. For each query variation, find max_score\")\n",
        "    print(\"2. Normalize: normalized_score = original_score / max_score\")\n",
        "    print(\"3. Sum normalized scores across all query variations\")\n",
        "    print(\"4. Documents with consistently high scores across queries win\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7f23T5v6Ys7",
        "outputId": "9ac17ef9-6515-48d0-fbf3-777b6c60ebc7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "6.2 RELATIVE SCORE FUSION MODE DEMONSTRATION\n",
            "============================================================\n",
            "Testing QueryFusionRetriever with relative_score mode:\n",
            "This mode preserves score magnitudes while normalizing across query variations\n",
            "\n",
            "Query: What are the main approaches to machine learning?\n",
            "QueryFusionRetriever with relative_score will:\n",
            "1. Generate query variations\n",
            "2. Normalize scores within each variation (score/max_score)\n",
            "3. Combine normalized scores\n",
            "\n",
            "Relative Score Fusion Results:\n",
            "1. Combined Relative Score: 0.7584\n",
            "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
            "\n",
            "2. Combined Relative Score: 0.5261\n",
            "   Text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs....\n",
            "\n",
            "3. Combined Relative Score: 0.3650\n",
            "   Text: Reinforcement learning is a type of machine learning where agents learn to make decisions through re...\n",
            "\n",
            "Relative Score Benefits in Query Fusion:\n",
            "- Preserves confidence information from embedding model\n",
            "- Ensures fair contribution from each query variation\n",
            "- More interpretable than rank-only methods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DISTRIBUTION-BASED SCORE FUSION\n",
        "\n",
        "print(\"=\" * 60)#https://docs.llamaindex.ai/en/stable/examples/retrievers/relative_score_dist_fusion/\n",
        "print(\"6.3 DISTRIBUTION-BASED SCORE FUSION MODE DEMONSTRATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "base_retriever = lab.vector_index.as_retriever(similarity_top_k=8)\n",
        "\n",
        "print(\"Testing QueryFusionRetriever with dist_based_score mode:\")\n",
        "print(\"This mode uses statistical analysis for the most sophisticated score fusion\")\n",
        "\n",
        "# Use the same query for consistency across all fusion modes\n",
        "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
        "\n",
        "try:\n",
        "    # Create query fusion retriever with distribution-based mode\n",
        "    dist_fusion = QueryFusionRetriever(\n",
        "        [base_retriever],\n",
        "        similarity_top_k=3,\n",
        "        num_queries=3,\n",
        "        mode=\"dist_based_score\",\n",
        "        use_async=False,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(\"QueryFusionRetriever with dist_based_score will:\")\n",
        "    print(\"1. Generate query variations\")\n",
        "    print(\"2. Analyze score distributions for each variation\")\n",
        "    print(\"3. Apply statistical normalization (z-score, percentiles)\")\n",
        "    print(\"4. Combine with distribution-aware weighting\")\n",
        "\n",
        "    nodes = dist_fusion.retrieve(query)\n",
        "\n",
        "    print(f\"\\nDistribution-Based Fusion Results:\")\n",
        "    for i, node in enumerate(nodes, 1):\n",
        "        print(f\"{i}. Statistically Normalized Score: {node.score:.4f}\")\n",
        "        print(f\"   Text: {node.text[:100]}...\")\n",
        "        print()\n",
        "\n",
        "    print(\"Distribution-Based Benefits in Query Fusion:\")\n",
        "    print(\"- Accounts for score distribution differences between query variations\")\n",
        "    print(\"- Statistically robust against outliers and noise\")\n",
        "    print(\"- Adapts weighting based on query variation reliability\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"QueryFusionRetriever error: {e}\")\n",
        "    print(\"Demonstrating Distribution-Based concept manually...\")\n",
        "\n",
        "    if not SCIPY_AVAILABLE:\n",
        "        print(\"‚ö†Ô∏è Full statistical analysis requires scipy\")\n",
        "\n",
        "    # Manual demonstration with query variations derived from the main query\n",
        "    query_variations = [\n",
        "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
        "        \"machine learning approaches and methods\",\n",
        "        \"different ML techniques and algorithms\"\n",
        "    ]\n",
        "\n",
        "    print(\"Manual Distribution-Based Fusion with Query Variations:\")\n",
        "    all_results = {}\n",
        "    variation_stats = []\n",
        "\n",
        "    # Step 1: Collect results and analyze distributions\n",
        "    for i, query_var in enumerate(query_variations):\n",
        "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
        "        nodes = base_retriever.retrieve(query_var)\n",
        "        scores = [node.score or 0 for node in nodes]\n",
        "\n",
        "        # Calculate distribution statistics\n",
        "        mean_score = np.mean(scores) if scores else 0\n",
        "        std_score = np.std(scores) if len(scores) > 1 else 1\n",
        "        min_score = np.min(scores) if scores else 0\n",
        "        max_score = np.max(scores) if scores else 1\n",
        "\n",
        "        stats_info = {\n",
        "            'mean': mean_score,\n",
        "            'std': std_score,\n",
        "            'min': min_score,\n",
        "            'max': max_score,\n",
        "            'nodes': nodes,\n",
        "            'scores': scores\n",
        "        }\n",
        "        variation_stats.append(stats_info)\n",
        "\n",
        "        print(f\"Distribution stats: mean={mean_score:.3f}, std={std_score:.3f}\")\n",
        "        print(f\"Score range: [{min_score:.3f}, {max_score:.3f}]\")\n",
        "\n",
        "        # Apply z-score normalization\n",
        "        for node, score in zip(nodes, scores):\n",
        "            node_id = node.node.node_id\n",
        "\n",
        "            # Z-score normalization\n",
        "            if std_score > 0:\n",
        "                z_score = (score - mean_score) / std_score\n",
        "            else:\n",
        "                z_score = 0\n",
        "\n",
        "            # Convert to [0,1] using sigmoid\n",
        "            normalized_score = 1 / (1 + np.exp(-z_score))\n",
        "\n",
        "            if node_id not in all_results:\n",
        "                all_results[node_id] = {\n",
        "                    'node': node,\n",
        "                    'combined_score': 0,\n",
        "                    'contributions': []\n",
        "                }\n",
        "\n",
        "            all_results[node_id]['combined_score'] += normalized_score\n",
        "            all_results[node_id]['contributions'].append({\n",
        "                'query': i,\n",
        "                'original': score,\n",
        "                'z_score': z_score,\n",
        "                'normalized': normalized_score\n",
        "            })\n",
        "\n",
        "    # Step 2: Sort by combined distribution-based score\n",
        "    sorted_results = sorted(\n",
        "        all_results.values(),\n",
        "        key=lambda x: x['combined_score'],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\nCombined Distribution-Based Results (top 3):\")\n",
        "    for i, result in enumerate(sorted_results[:3], 1):\n",
        "        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n",
        "        print(f\"   Statistical breakdown:\")\n",
        "        for contrib in result['contributions']:\n",
        "            print(f\"     Query {contrib['query']}: {contrib['original']:.3f} ‚Üí \"\n",
        "                  f\"z={contrib['z_score']:.2f} ‚Üí {contrib['normalized']:.3f}\")\n",
        "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
        "        print()\n",
        "\n",
        "    print(\"Distribution-Based Process:\")\n",
        "    print(\"1. Calculate mean and std for each query variation\")\n",
        "    print(\"2. Z-score normalize: z = (score - mean) / std\")\n",
        "    print(\"3. Sigmoid transform: normalized = 1 / (1 + exp(-z))\")\n",
        "    print(\"4. Sum normalized scores across variations\")\n",
        "    print(\"5. Results reflect statistical significance across all query forms\")\n",
        "\n",
        "# Show fusion mode comparison summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FUSION MODES COMPARISON SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(\"All three modes tested with the same query for direct comparison:\")\n",
        "print(f\"Query: {query}\")\n",
        "print()\n",
        "print(\"Mode Characteristics:\")\n",
        "print(\"‚Ä¢ RRF (reciprocal_rerank): Most robust, rank-based, scale-invariant\")\n",
        "print(\"‚Ä¢ Relative Score: Preserves confidence, normalizes by max score\")\n",
        "print(\"‚Ä¢ Distribution-Based: Most sophisticated, statistical normalization\")\n",
        "print()\n",
        "print(\"Choose based on your use case:\")\n",
        "print(\"- Production stability ‚Üí RRF\")\n",
        "print(\"- Score interpretability ‚Üí Relative Score\")\n",
        "print(\"- Statistical robustness ‚Üí Distribution-Based\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ9CfHth-TI0",
        "outputId": "30ed22cf-8651-498f-d05e-e3c8845b1727"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "6.3 DISTRIBUTION-BASED SCORE FUSION MODE DEMONSTRATION\n",
            "============================================================\n",
            "Testing QueryFusionRetriever with dist_based_score mode:\n",
            "This mode uses statistical analysis for the most sophisticated score fusion\n",
            "\n",
            "Query: What are the main approaches to machine learning?\n",
            "QueryFusionRetriever with dist_based_score will:\n",
            "1. Generate query variations\n",
            "2. Analyze score distributions for each variation\n",
            "3. Apply statistical normalization (z-score, percentiles)\n",
            "4. Combine with distribution-aware weighting\n",
            "\n",
            "Distribution-Based Fusion Results:\n",
            "1. Statistically Normalized Score: 0.7287\n",
            "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
            "\n",
            "2. Statistically Normalized Score: 0.6149\n",
            "   Text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs....\n",
            "\n",
            "3. Statistically Normalized Score: 0.5632\n",
            "   Text: Reinforcement learning is a type of machine learning where agents learn to make decisions through re...\n",
            "\n",
            "Distribution-Based Benefits in Query Fusion:\n",
            "- Accounts for score distribution differences between query variations\n",
            "- Statistically robust against outliers and noise\n",
            "- Adapts weighting based on query variation reliability\n",
            "\n",
            "============================================================\n",
            "FUSION MODES COMPARISON SUMMARY\n",
            "============================================================\n",
            "All three modes tested with the same query for direct comparison:\n",
            "Query: What are the main approaches to machine learning?\n",
            "\n",
            "Mode Characteristics:\n",
            "‚Ä¢ RRF (reciprocal_rerank): Most robust, rank-based, scale-invariant\n",
            "‚Ä¢ Relative Score: Preserves confidence, normalizes by max score\n",
            "‚Ä¢ Distribution-Based: Most sophisticated, statistical normalization\n",
            "\n",
            "Choose based on your use case:\n",
            "- Production stability ‚Üí RRF\n",
            "- Score interpretability ‚Üí Relative Score\n",
            "- Statistical robustness ‚Üí Distribution-Based\n"
          ]
        }
      ]
    }
  ]
}